{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chart Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "\n",
    "#colorbrewer2 Dark2 qualitative color table\n",
    "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
    "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
    "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
    "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
    "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
    "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
    "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843)]\n",
    "\n",
    "rcParams['figure.figsize'] = (10, 6)\n",
    "rcParams['figure.dpi'] = 150\n",
    "rcParams['axes.color_cycle'] = dark2_colors\n",
    "rcParams['lines.linewidth'] = 2\n",
    "rcParams['axes.facecolor'] = 'white'\n",
    "rcParams['font.size'] = 14\n",
    "rcParams['patch.edgecolor'] = 'white'\n",
    "rcParams['patch.facecolor'] = dark2_colors[0]\n",
    "#rcParams['font.family'] = 'sans-serif'\n",
    "#rcParams['font.sans-serif'] = 'Helvetica'\n",
    "\n",
    "\n",
    "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
    "    \"\"\"\n",
    "    Minimize chartjunk by stripping out unnecesasry plot borders and axis ticks\n",
    "    \n",
    "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
    "    \"\"\"\n",
    "    ax = axes or plt.gca()\n",
    "    ax.spines['top'].set_visible(top)\n",
    "    ax.spines['right'].set_visible(right)\n",
    "    ax.spines['left'].set_visible(left)\n",
    "    ax.spines['bottom'].set_visible(bottom)\n",
    "    \n",
    "    #turn off all ticks\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    \n",
    "    #now re-enable visibles\n",
    "    if top:\n",
    "        ax.xaxis.tick_top()\n",
    "    if bottom:\n",
    "        ax.xaxis.tick_bottom()\n",
    "    if left:\n",
    "        ax.yaxis.tick_left()\n",
    "    if right:\n",
    "        ax.yaxis.tick_right()\n",
    "        \n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "#utility functions\n",
    "\n",
    "def histogram_style():\n",
    "    remove_border(left=False)\n",
    "    plt.grid(False)\n",
    "    plt.grid(axis='y', color='w', linestyle='-', lw=1)\n",
    "    \n",
    "def histogram_labels(xlabel, ylabel, title, loc):    \n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend(frameon=False, loc=loc)\n",
    "    \n",
    "def histogram_settings(xlabel, ylabel, title, loc = 'upper left'):\n",
    "    histogram_style() \n",
    "    histogram_labels(xlabel, ylabel, title, loc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "def points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=True, colorscale=cmap_light, cdiscrete=cmap_bold, alpha=0.1, psize=10, zfunc=False, predicted=False):\n",
    "    h = .02\n",
    "    X=np.concatenate((Xtr, Xte))\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "\n",
    "    #plt.figure(figsize=(10,6))\n",
    "    if zfunc:\n",
    "        p0 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n",
    "        p1 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "        Z=zfunc(p0, p1)\n",
    "    else:\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    ZZ = Z.reshape(xx.shape)\n",
    "    if mesh:\n",
    "        plt.pcolormesh(xx, yy, ZZ, cmap=cmap_light, alpha=alpha, axes=ax)\n",
    "    if predicted:\n",
    "        showtr = clf.predict(Xtr)\n",
    "        showte = clf.predict(Xte)\n",
    "    else:\n",
    "        showtr = ytr\n",
    "        showte = yte\n",
    "    ax.scatter(Xtr[:, 0], Xtr[:, 1], c=showtr-1, cmap=cmap_bold, s=psize, alpha=alpha,edgecolor=\"k\")\n",
    "    # and testing points\n",
    "    ax.scatter(Xte[:, 0], Xte[:, 1], c=showte-1, cmap=cmap_bold, alpha=alpha, marker=\"s\", s=psize+10)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    return ax,xx,yy\n",
    "\n",
    "def points_plot_prob(ax, Xtr, Xte, ytr, yte, clf, colorscale=cmap_light, cdiscrete=cmap_bold, ccolor=cm, psize=10, alpha=0.1):\n",
    "    ax,xx,yy = points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=False, colorscale=colorscale, cdiscrete=cdiscrete, psize=psize, alpha=alpha, predicted=True) \n",
    "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=ccolor, alpha=.2, axes=ax)\n",
    "    cs2 = plt.contour(xx, yy, Z, cmap=ccolor, alpha=.6, axes=ax)\n",
    "    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize=14, axes=ax)\n",
    "    return ax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful methods to train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "def cv_optimize(clf, parameters, Xtrain, ytrain, n_folds=5)\n",
    "Performs cross validation of n_folds using a list of values for a chosen parameter (say regularization parameter, C) and returns\n",
    "the parameter with the best performance\n",
    "\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "clf : Model (eg: LogisticRegression())\n",
    "parameters: List of parameter values to test eg. {\"C\": [0.01, 0.1, 1, 10, 100]}\n",
    "Xtrain: Training features\n",
    "ytrain: Training labels\n",
    "n_folds: No of cross validation folds eg. 5\n",
    "  \n",
    "Returns\n",
    "-------\n",
    "best: Best parameter from the values given\n",
    "    \n",
    "\"\"\"\n",
    "def cv_optimize(clf, parameters, Xtrain, ytrain, n_folds=5):\n",
    "    gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds)\n",
    "    gs.fit(Xtrain, ytrain)\n",
    "    print \"BEST PARAMS\", gs.best_params_\n",
    "    best = gs.best_estimator_\n",
    "    return best\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "def do_classify(clf, parameters, indf, featurenames, targetname, target1val, standardize=False, train_size=0.8)\n",
    "Trains a model and computes the accuracy on train and test data. Sub-tasks included:\n",
    "1. Subset data frame by the given features\n",
    "2. Split in to train and test data\n",
    "3. Cross validation using cv_optimize\n",
    "4. Fit the model\n",
    "5. Compute and print train and test accuracy\n",
    "\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "clf : Model (eg: LogisticRegression())\n",
    "parameters: List of parameter values to test eg. {\"C\": [0.01, 0.1, 1, 10, 100]}\n",
    "indf: data frame\n",
    "featurenames: List of feature names\n",
    "targetname: Name of prediction variable\n",
    "target1val: Value of prediction variable\n",
    "  \n",
    "Returns\n",
    "-------\n",
    "clf, Xtrain, ytrain, Xtest, ytest\n",
    "\"\"\"\n",
    "def do_classify(clf, parameters, indf, featurenames, targetname, target1val, standardize=False, train_size=0.8):\n",
    "    subdf=indf[featurenames]\n",
    "    if standardize:\n",
    "        subdfstd=(subdf - subdf.mean())/subdf.std()\n",
    "    else:\n",
    "        subdfstd=subdf\n",
    "    X=subdfstd.values\n",
    "    y=(indf[targetname].values==target1val)*1\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size=train_size)\n",
    "    clf = cv_optimize(clf, parameters, Xtrain, ytrain)\n",
    "    clf=clf.fit(Xtrain, ytrain)\n",
    "    training_accuracy = clf.score(Xtrain, ytrain)\n",
    "    test_accuracy = clf.score(Xtest, ytest)\n",
    "    print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
    "    print \"Accuracy on test data:     %0.2f\" % (test_accuracy)\n",
    "    return clf, Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import simplejson as json\n",
    "with open('./data/us_state_map.geojson','r') as fp:\n",
    "    statedata = json.load(fp)\n",
    "with open('./data/us_county_map.geojson','r') as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get json data from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "def get_senate_vote(vote):\n",
    "    url = 'https://www.govtrack.us/data/congress/113/votes/2013/s'+str(vote)+'/data.json'\n",
    "    vote_data = requests.get(url)\n",
    "    return json.loads(vote_data.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data from a url using beautiful soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code here\n",
    "def get_all_votes():\n",
    "    url = 'http://www.govtrack.us/data/congress/113/votes/2013'\n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.content, \"lxml\")\n",
    "    s_votes = [a['href'][1:-1] for a in soup.find_all('a')          \n",
    "     if a['href'].startswith('s')]\n",
    "    return [get_senate_vote(vote) for vote in s_votes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions and tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fulldf=pd.read_csv(\"bigdf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recompute dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to re-compute review counts and averages whenever you subset a reviews data frame. We'll use it soon to construct a smaller, more computationally tractable data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recompute_frame(ldf):\n",
    "    \"\"\"\n",
    "    takes a dataframe ldf, makes a copy of it, and returns the copy\n",
    "    with all averages and review counts recomputed\n",
    "    this is used when a frame is subsetted.\n",
    "    \"\"\"\n",
    "    ldfu=ldf.groupby('user_id')\n",
    "    ldfb=ldf.groupby('business_id')\n",
    "    user_avg=ldfu.stars.mean()\n",
    "    user_review_count=ldfu.review_id.count()\n",
    "    business_avg=ldfb.stars.mean()\n",
    "    business_review_count=ldfb.review_id.count()\n",
    "    nldf=ldf.copy()\n",
    "    nldf.set_index(['business_id'], inplace=True)\n",
    "    nldf['business_avg']=business_avg\n",
    "    nldf['business_review_count']=business_review_count\n",
    "    nldf.reset_index(inplace=True)\n",
    "    nldf.set_index(['user_id'], inplace=True)\n",
    "    nldf['user_avg']=user_avg\n",
    "    nldf['user_review_count']=user_review_count\n",
    "    nldf.reset_index(inplace=True)\n",
    "    return nldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "smalldf = fulldf[(fulldf.business_review_count > 150) & (fulldf.user_review_count > 60)] \n",
    "smalldf_new = recompute_frame(smalldf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Common Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common support is an important concept, as for each pair of restaurants, its the number of people who reviewed both. It will be used to modify similarity between restaurants. If the common support is low, the similarity is less believable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "restaurants=smalldf.business_id.unique()  #get all the unique restaurant ids\n",
    "supports=[]\n",
    "for i,rest1 in enumerate(restaurants):  # first restaurant (rest1) in the pair\n",
    "    for j,rest2 in enumerate(restaurants): # second restaurant(rest2) in the pair\n",
    "        if  i < j: #skip pairing same restaurants and forming duplicate pairs\n",
    "            rest1_reviewers = smalldf[smalldf.business_id==rest1].user_id.unique() #find all unique users who reviewed restaurant 1\n",
    "            rest2_reviewers = smalldf[smalldf.business_id==rest2].user_id.unique() #find all unique users who reviewed restaurant 2\n",
    "            common_reviewers = set(rest1_reviewers).intersection(rest2_reviewers) # find common reviewers by taking intersection\n",
    "            supports.append(len(common_reviewers)) # add the no of common reviewers to list\n",
    "print \"Mean support is:\",np.mean(supports)\n",
    "plt.hist(supports) #plot hist of list\n",
    "histogram_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return X (column value) from a df given Y (another column value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test business id\n",
    "testbizid=\"eIxSLxzIlfExI6vgAbn2JA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return business name from a df given id\n",
    "def biznamefromid(df, theid):\n",
    "    return df['biz_name'][df['business_id']==theid].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print testbizid, biznamefromid(smalldf,testbizid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Simulating Elections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictwise = pd.read_csv('data/predictwise.csv').set_index('States')\n",
    "predictwise.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code here\n",
    "def simulate_election(model, n_sim):\n",
    "    simulations = np.random.uniform(size= (51,n_sim))\n",
    "    obama_votes = (simulations < model.Obama.values.reshape(-1,1))* model.Votes.values.reshape(-1,1)\n",
    "    results  = obama_votes.sum(axis = 0)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = simulate_election(predictwise, 10000)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "def plot_simulation(simulation):    \n",
    "    plt.hist(simulation, bins=np.arange(200, 538, 1), \n",
    "             label='simulations', align='left', normed=True)\n",
    "    plt.axvline(332, 0, .5, color='r', label='Actual Outcome')\n",
    "    plt.axvline(269, 0, .5, color='k', label='Victory Threshold')\n",
    "    p05 = np.percentile(simulation, 5.)\n",
    "    p95 = np.percentile(simulation, 95.)\n",
    "    iq = int(p95 - p05)\n",
    "    pwin = ((simulation >= 269).mean() * 100)\n",
    "    plt.title(\"Chance of Obama Victory: %0.2f%%, Spread: %d votes\" % (pwin, iq))\n",
    "    plt.legend(frameon=False, loc='upper left')\n",
    "    plt.xlabel(\"Obama Electoral College Votes\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    remove_border()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_simulation(result)\n",
    "plt.xlim(240,380)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "critics = pd.read_csv('critics.csv') # Rotten Tomatoes Top Critics Reviews Data for about 3000 movies\n",
    "\n",
    "# clean data\n",
    "critics = critics[~critics.quote.isnull()]\n",
    "critics = critics[critics.fresh.notnull()]\n",
    "critics = critics[critics.quote.str.len() > 0]\n",
    "critics = critics.reset_index(drop = True)\n",
    "critics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the distribution of number of reviews per reviewer look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(critics.groupby('critic').rtid.count(),log = True, bins=range(20), edgecolor='white')\n",
    "plt.xlabel(\"Number of reviews per critic\")\n",
    "plt.ylabel(\"N\")\n",
    "histogram_style()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the critics with > 100 reviews, plot the distribution of average \"freshness\" rating per critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = critics.copy()\n",
    "df['fresh'] = df.fresh == 'fresh'\n",
    "\n",
    "grp = df.groupby('critic')\n",
    "counts = grp.rtid.count()\n",
    "means = grp.fresh.mean()\n",
    "means[counts > 100].hist(bins=10, edgecolor='w', lw=1)\n",
    "plt.xlabel(\"Average rating per critic\")\n",
    "plt.ylabel(\"N\")\n",
    "plt.yticks([0, 2, 4, 6, 8, 10])\n",
    "histogram_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the original movies dataframe, plot the rotten tomatoes Top Critics Rating as a function of year. Overplot the average for each year, ignoring the score=0 examples (some of these are missing data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the data\n",
    "from io import StringIO  \n",
    "movie_txt = requests.get('https://raw.github.com/cs109/cs109_data/master/movies.dat').text\n",
    "movie_file = StringIO(movie_txt) # treat a string like a file\n",
    "movies = pd.read_csv(movie_file, delimiter='\\t')\n",
    "movies = movies.dropna()\n",
    "movies = movies.reset_index(drop = True)\n",
    "\n",
    "# process data\n",
    "data = movies[['year','rtTopCriticsRating']]\n",
    "#data = data.convert_objects(convert_numeric=True) #deprecated\n",
    "data = data.apply(pd.to_numeric)\n",
    "data = data[(data.rtTopCriticsRating > 0.00)]\n",
    "means  = data.groupby('year').mean()\n",
    "\n",
    "# plotting\n",
    "plt.plot(data['year'], data['rtTopCriticsRating'], 'o', mec = 'none', alpha = .5, label = 'Data' )\n",
    "plt.plot(means.index, means['rtTopCriticsRating'], '-',  label = 'Yearly Average' )\n",
    "plt.legend(loc ='lower left', frameon = False)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Average Score\")\n",
    "remove_border()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows a trend towards a lower average score, as well as a greater abundance of low scores, with time. This is probably at least partially a selection effect -- Rotten Tomatoes probably doesn't archive reviews for all movies, especially ones that came out before the website existed. Thus, reviews of old movies are more often \"the classics\". Mediocre old movies have been partially forgotten, and are underrepresented in the data. Other questions worth exploring:\n",
    "Does the down trend mean:\n",
    "* the quality of movies has dropped ?\n",
    "* the top critics became more stringent in giving higher ratings?\n",
    "* more top critics with stringent ratings entered the scene?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"01_heights_weights_genders.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_l, Xtrain_l, ytrain_l, Xtest_l, ytest_l  = do_classify(LogisticRegression(), {\"C\": [0.01, 0.1, 1, 10, 100]}, df, ['Weight', 'Height'], 'Gender','Male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize =(12,8))\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Height\")\n",
    "ax=plt.gca()\n",
    "points_plot(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clf_l, alpha=0.2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let us plot the probabilities obtained from predict_proba, overlayed on the samples with their true labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize =(12,8))\n",
    "plt.grid(True)\n",
    "ax=plt.gca()\n",
    "points_plot_prob(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clf_l, psize=20, alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.lda import LDA\n",
    "clflda = LDA(solver=\"svd\", store_covariance=True)\n",
    "clflda.fit(Xtrain_l, ytrain_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from REF\n",
    "from scipy import linalg\n",
    "\n",
    "def plot_ellipse(splot, mean, cov, color):\n",
    "    v, w = linalg.eigh(cov)\n",
    "    u = w[0] / linalg.norm(w[0])\n",
    "    angle = np.arctan(u[1] / u[0])\n",
    "    angle = 180 * angle / np.pi  # convert to degrees\n",
    "    # filled Gaussian at 2 standard deviation\n",
    "    ell = mpl.patches.Ellipse(mean, 2 * v[0] ** 0.5, 2 * v[1] ** 0.5,\n",
    "                              180 + angle, color=color, lw=3, fill=False)\n",
    "    ell.set_clip_box(splot.bbox)\n",
    "    ell1 = mpl.patches.Ellipse(mean, 1 * v[0] ** 0.5, 1 * v[1] ** 0.5,\n",
    "                              180 + angle, color=color, lw=3, fill=False)\n",
    "    ell1.set_clip_box(splot.bbox)\n",
    "    ell3 = mpl.patches.Ellipse(mean, 3 * v[0] ** 0.5, 3 * v[1] ** 0.5,\n",
    "                              180 + angle, color=color, lw=3, fill=False)\n",
    "    ell3.set_clip_box(splot.bbox)\n",
    "    #ell.set_alpha(0.2)\n",
    "    splot.add_artist(ell)\n",
    "    splot.add_artist(ell1)\n",
    "    splot.add_artist(ell3)\n",
    "\n",
    "\n",
    "    #splot.set_xticks(())\n",
    "    #splot.set_yticks(())\n",
    "def plot_lda_cov(lda, splot):\n",
    "    plot_ellipse(splot, lda.means_[0], lda.covariance_, 'red')\n",
    "    plot_ellipse(splot, lda.means_[1], lda.covariance_, 'blue')\n",
    "#plt.bivariate_normal(X, Y, sigmax=1.0, sigmay=1.0, mux=0.0, muy=0.0, sigmaxy=0.0)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize =(12,8))\n",
    "ax=plt.gca()\n",
    "spl,_,_=points_plot(ax,Xtrain_l, Xtest_l, ytrain_l, ytest_l, clflda)\n",
    "plot_lda_cov(clflda, spl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfd=pd.read_csv(\"https://dl.dropboxusercontent.com/u/75194/pima.csv\")\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "features=['npreg', 'bmi','diaped', 'age', 'glconc','insulin']\n",
    "X=dfd[features].values\n",
    "y=dfd.diabetes.values\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf=GaussianNB()\n",
    "itr,ite=train_test_split(range(y.shape[0]))\n",
    "Xtr=X[itr]\n",
    "Xte=X[ite]\n",
    "ytr=y[itr]\n",
    "yte=y[ite]\n",
    "clf.fit(Xtr,ytr)\n",
    "print \"Frac of mislabeled points\",float((yte != clf.predict(Xte)).sum())/yte.shape[0]\n",
    "confusion_matrix(clf.predict(Xte),yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calibration_plot(clf, xtest, ytest):\n",
    "    prob = clf.predict_proba(xtest)[:, 1]\n",
    "    outcome = ytest\n",
    "    data = pd.DataFrame(dict(prob=prob, outcome=outcome))\n",
    "\n",
    "    #group outcomes into bins of similar probability\n",
    "    bins = np.linspace(0, 1, 20)\n",
    "    cuts = pd.cut(prob, bins)\n",
    "    binwidth = bins[1] - bins[0]\n",
    "    \n",
    "    #freshness ratio and number of examples in each bin\n",
    "    cal = data.groupby(cuts).outcome.agg(['mean', 'count'])\n",
    "    cal['pmid'] = (bins[:-1] + bins[1:]) / 2\n",
    "    cal['sig'] = np.sqrt(cal.pmid * (1 - cal.pmid) / cal['count'])\n",
    "        \n",
    "    #the calibration plot\n",
    "    ax = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    p = plt.errorbar(cal.pmid, cal['mean'], cal['sig'])\n",
    "    plt.plot(cal.pmid, cal.pmid, linestyle='--', lw=1, color='k')\n",
    "    plt.ylabel(\"Empirical Fraction\")\n",
    "\n",
    "    \n",
    "    #the distribution of P(fresh)\n",
    "    ax = plt.subplot2grid((3, 1), (2, 0), sharex=ax)\n",
    "    #calsum = cal['count'].sum()\n",
    "    plt.bar(left=cal.pmid - binwidth / 2, height=cal['count'],\n",
    "            width=.95 * (bins[1] - bins[0]),\n",
    "            fc=p[0].get_color())\n",
    "    plt.xlabel(\"Classifier Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calibration_plot(clf, Xte, yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Coin tosses: Binomial-Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 9))\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "beta = stats.beta\n",
    "n_trials = [0, 1, 2, 3, 4, 5, 8, 15, 50, 500]\n",
    "data = stats.bernoulli.rvs(0.5, size=n_trials[-1])\n",
    "x = np.linspace(0, 1, 100)\n",
    "\n",
    "# For the already prepared, I'm using Binomial's conj. prior.\n",
    "for k, N in enumerate(n_trials):\n",
    "    sx = plt.subplot(len(n_trials) / 2, 2, k + 1)\n",
    "    plt.xlabel(\"$p$, probability of heads\") \\\n",
    "        if k in [0, len(n_trials) - 1] else None\n",
    "    plt.setp(sx.get_yticklabels(), visible=False)\n",
    "    heads = data[:N].sum()\n",
    "    #posterior distribution.\n",
    "    y = beta.pdf(x, 1 + heads, 1 + N - heads)\n",
    "    plt.plot(x, y, label=\"observe %d tosses,\\n %d heads\" % (N, heads))\n",
    "    plt.fill_between(x, 0, y, color=\"#348ABD\", alpha=0.4)\n",
    "    plt.vlines(0.5, 0, 4, color=\"k\", linestyles=\"--\", lw=1)\n",
    "\n",
    "    leg = plt.legend()\n",
    "    leg.get_frame().set_alpha(0.4)\n",
    "    plt.autoscale(tight=True)\n",
    "\n",
    "\n",
    "plt.suptitle(\"Bayesian updating of posterior probabilities\",\n",
    "             y=1.02,\n",
    "             fontsize=14)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian with known $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = [16.4, 17.0, 17.2, 17.4, 18.2, 18.2, 18.2, 19.9, 20.8]\n",
    "# Prior mean\n",
    "mu_prior = 19.5\n",
    "# prior std\n",
    "tau = 10 \n",
    "N = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data Quantities\n",
    "sig = np.std(Y) # assume that is the value of KNOWN sigma (in the likelihood)\n",
    "mu_data = np.mean(Y)\n",
    "n = len(Y)\n",
    "kappa = sig**2 / tau**2\n",
    "sig_post =np.sqrt(1./( 1./tau**2 + n/sig**2));\n",
    "# posterior mean\n",
    "mu_post = kappa / (kappa + n) *mu_prior + n/(kappa+n)* mu_data\n",
    "\n",
    "#samples\n",
    "theta_prior = np.random.normal(loc=mu_prior, scale=tau, size=N);\n",
    "theta_post = np.random.normal(loc=mu_post, scale=sig_post, size=N);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(theta_post, bins=30, alpha=0.9, label=\"posterior\");\n",
    "plt.hist(theta_prior, bins=30, alpha=0.2, label=\"prior\");\n",
    "plt.xlim([10, 30])\n",
    "plt.xlabel(\"wing length (mm)\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use of python *class* to implement a database of similarities: See CS109 HW4\n",
    "2. Print formatting: See CS109 HW4 or HW3\n",
    "3. Use of JSON and complex dictionaries: HW5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
